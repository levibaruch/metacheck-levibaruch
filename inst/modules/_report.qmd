---
title: MetaCheck Report
subtitle: "To Err is Human: An Empirical Investigation"
date: 2025-12-03
format:
  html:
    theme: flatly
    toc: true
    toc-location: right
    page-layout: full
    title-block-banner: true
    link-external-newwindow: true
    embed-resources: true
---

<style>
  .na::before     { content: '‚ö™Ô∏è '; }
  .fail::before   { content: '‚ö´Ô∏è '; }
  .info::before   { content: 'üîµ '; }
  .red::before    { content: 'üî¥ '; }
  .yellow::before { content: 'üü° '; }
  .green::before  { content: 'üü¢ '; }
  section::before { content: '' !important; }
  details summary { font-size: 150%; padding: 0.5em 0; }
</style>

::: {.column-margin}
[MetaCheck](http://www.scienceverse.org/metacheck) version 0.0.0.9059<br><br>

üü¢ no problems detected;<br>
üü° something to check;<br>
üî¥ possible problems detected;<br>
üîµ informational only;<br>
‚ö™Ô∏è not applicable;<br>
‚ö´Ô∏è check failed
:::

## Summary

- [Exact P-Values](#exact-p-values){.red}: Exact p summary  
- [Marginal Significance](#marginal-significance){.red}: Marginal summary this one is a bit longer to span multiple line in the report  
- [Missing Effect Sizes in t-tests and F-tests](#missing-effect-sizes-in-t-tests-and-f-tests){.red}: We found 1 t-test and/or F-test where effect sizes are not reported.  
- [StatCheck](#statcheck){.red}: 1 possible errors in test statistics  
- [RetractionWatch](#retractionwatch){.yellow}: You cited 1 paper in the Retraction Watch database  
- [Reference Consistency](#reference-consistency){.red}:   

## Exact P-Values {.red}

We found 1 imprecise *p* value. Reporting *p* values imprecisely (e.g., *p* < .05) reduces transparency, reproducibility, and re-use (e.g., in *p* value meta-analyses). Best practice is to report exact p-values with three decimal places (e.g., *p* = .032) unless *p* values are smaller than 0.001, in which case you can use *p* < .001.

```{r}
#| echo: false
# table data
table <- structure(list("There was no effect of experience on the reduction in errors when using the tool (p > .05), as the correlation was non-significant."), names = "", row.names = c(NA, 
-1L), class = c("tbl_df", "tbl", "data.frame"))

# table options
options <- list(dom = "t", ordering = FALSE )

# display table
DT::datatable(table, options, selection = "none", rownames = FALSE)
```

<details>
  <summary>Learn More</summary>
  <div>
The APA manual states: Report exact *p* values (e.g., *p* = .031) to two or three decimal places. However, report *p* values less than .001 as *p* < .001. However, 2 decimals is too imprecise for many use-cases (e.g., a *p* value meta-analysis), so report *p* values with three digits.

American Psychological Association. (2020). Publication manual of the American Psychological Association 2020: the official guide to APA style (7th ed.). American Psychological Association.
  </div>
</details>

## Marginal Significance {.red}

You described effects with terms related to 'marginally significant'. If *p* values above 0.05 are interpreted as an effect, you inflate the alpha level, and increase the Type 1 error rate. If a *p* value is higher than the prespecified alpha level, it should be interpreted as a non-significant result.

```{r}
#| echo: false
# table data
table <- structure(list(section = c("abstract", "results"), text = c("The paper shows examples of (1) open and closed OSF links; (2a) citation of retracted papers, (2b) citations without a doi, (2c) citations with Pubpeer comments, (2d) citations in the FORTT replication database, and (2e) missing/mismatched/incorrect citations and references; (3a) R files with code on GitHub that do not load libraries in one location, (3b) load files that are not shared in the repository, (3c) lack comments, and (3d) have hard-coded files, (4) imprecise reporting of non-significant p-values; (5) tests with and without effect sizes, (6) use of \"marginally significant\" to describe non-significant findings, and (7) retrieving information from preregistrations.", 
"On average researchers in the experimental condition found the app marginally significantly more useful (M = 5.06) than researchers in the control condition found the checklist (M = 4.5), t(97.2) = -1.96, p = 0.152."
)), row.names = c(NA, -2L), class = c("tbl_df", "tbl", "data.frame"
))

# table options
options <- list(dom = "t", ordering = FALSE )

# display table
DT::datatable(table, options, selection = "none", rownames = FALSE)
```

<details>
  <summary>Learn More</summary>
  <div>
For metascientific articles demonstrating the rate at which non-significant p-values are interpreted as marginally significant, see:

Olsson-Collentine, A., van Assen, M. A. L. M., & Hartgerink, C. H. J. (2019). The Prevalence of Marginally Significant Results in Psychology Over Time. Psychological Science, 30(4), 576‚Äì586. 

<https://doi.org/10.1177/0956797619830326>

For the list of terms used to identifify marginally significant results, see this blog post by Matthew Hankins:

<https://web.archive.org/web/20251001114321/https://mchankins.wordpress.com/2013/04/21/still-not-significant-2/>
  </div>
</details>

## Missing Effect Sizes in t-tests and F-tests {.red}

We found 1 t-test and/or F-test where effect sizes are not reported. We recommend checking the sentences below, and add any missing effect sizes.

The following sentences are missing effect sizes

```{r}
#| echo: false
# table data
table <- structure(list("On average researchers in the experimental condition found the app marginally significantly more useful (M = 5.06) than researchers in the control condition found the checklist (M = 4.5), t(97.2) = -1.96, p = 0.152."), names = "", class = "data.frame", row.names = c(NA, 
-1L))

# table options
options <- list(dom = "t", ordering = FALSE )

# display table
DT::datatable(table, options, selection = "none", rownames = FALSE)
```

<details>
  <summary>Learn More</summary>
  <div>
**For metascientific articles demonstrating that effect sizes are often not reported**:

* Peng, C.-Y. J., Chen, L.-T., Chiang, H.-M., & Chiang, Y.-C. (2013). The Impact of APA and AERA Guidelines on Effect Size Reporting. Educational Psychology Review, 25(2), 157‚Äì209. doi:[10.1007/s10648-013-9218-2](https://doi.org/10.1007/s10648-013-9218-2).

For educational material on reporting effect sizes:

* [Guide to Effect Sizes and Confidence Intervals](https://matthewbjane.quarto.pub/guide-to-effect-sizes-and-confidence-intervals/)
  </div>
</details>

All detected and assessed stats:

```{r}
#| echo: false
# table data
table <- structure(list(Sentence = c("On average researchers in the experimental (app) condition made fewer mistakes (M = 9.12) than researchers in the control (checklist) condition (M = 10.9), t(97.7) = 2.9, p = 0.005, d = 0.59.", 
"On average researchers in the experimental condition found the app marginally significantly more useful (M = 5.06) than researchers in the control condition found the checklist (M = 4.5), t(97.2) = -1.96, p = 0.152."
), Section = c("method", "results"), "Effect Size" = c("d = 0.59", 
NA), "Reported Test" = c("t(97.7) = 2.9", "t(97.2) = -1.96"), 
    "Test Type" = c("t-test", "t-test")), row.names = c(NA, -2L
), class = c("tbl_df", "tbl", "data.frame"))

# table options
options <- list(dom = "t", ordering = FALSE )

# display table
DT::datatable(table, options, selection = "none", rownames = FALSE)
```

## StatCheck {.red}

We detected possible errors in test statistics. Note that as the accuracy of statcheck has only been validated for *t*-tests and *F*-tests. As Metacheck only uses validated modules, we only provide statcheck results for *t* tests and *F*-tests

```{r}
#| echo: false
# table data
table <- structure(list("Sentence / Text" = "On average researchers in the experimental condition found the app marginally significantly more useful (M = 5.06) than researchers in the control condition found the checklist (M = 4.5), t(97.2) = -1.96, p = 0.152.", 
    "Reported p" = 0.152, "Recomputed p" = 0.05285936397754, 
    Section = "results"), class = c("statcheck", "data.frame"
), row.names = 2L)

# table options
options <- list(dom = "t", ordering = FALSE )

# display table
DT::datatable(table, options, selection = "none", rownames = FALSE)
```

<details>
  <summary>Learn More</summary>
  <div>
For metascientific research on the validity of statcheck, and it's usefulness to prevent statistical reporting errors, see:<br><br>

Nuijten, M. B., van Assen, M. A. L. M., Hartgerink, C. H. J., Epskamp, S., & Wicherts, J. M. (2017). The Validity of the Tool ‚Äústatcheck‚Äù in Discovering Statistical Reporting Inconsistencies. PsyArXiv. doi: [10.31234/osf.io/tcxaja](https://doi.org/10.31234/osf.io/tcxaja)

Nuijten, M. B., & Wicherts, J. (2023). The effectiveness of implementing statcheck in the peer review process to avoid statistical reporting errors. PsyArXiv. doi: [10.31234/osf.io/bxau9](https://doi.org/10.31234/osf.io/bxau9)
  </div>
</details>

## RetractionWatch {.yellow}

You cited 1 paper in the Retraction Watch database (as of 2025-08-22). These may be retracted, have corrections, or expressions of concern.

 
```{r}
#| echo: false
# table data
table <- structure(list(doi = "10.1177/0956797614520714", "RW Status" = "Retraction"), class = "data.frame", row.names = c(NA, 
-1L))

# table options
options <- list(dom = "t", ordering = FALSE )

# display table
DT::datatable(table, options, selection = "none", rownames = FALSE)
```

## Reference Consistency {.red}

This module relies on Grobid correctly parsing the references. There may be some false positives.

There are cross-references that are not in the bibliography or bibliography entries not cross-referenced in the text
